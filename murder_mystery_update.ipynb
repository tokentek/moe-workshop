{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create your model client by passing in API key and endoint belov and run cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from autogen_ext.models.openai import AzureOpenAIChatCompletionClient\n",
    "\n",
    "API_KEY = \"\" # <---- Please enter your API key here\n",
    "ENDPOINT = \"\" # <---- Please enter your endpoint here\n",
    "\n",
    "def get_model_client_4o() -> AzureOpenAIChatCompletionClient:\n",
    "\n",
    "    return AzureOpenAIChatCompletionClient(\n",
    "    api_version=\"2025-01-01-preview\",\n",
    "    model=\"gpt-4o-2024-11-20\",\n",
    "    model_capabilities={\n",
    "    \"function_calling\": True,\n",
    "    \"json_output\": True,\n",
    "    \"vision\": True,\n",
    "    },\n",
    "    azure_endpoint=ENDPOINT,\n",
    "    api_key=API_KEY,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper functions for using agents with autogen framework "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports of autogen funcitons\n",
    "from autogen_agentchat.agents import AssistantAgent, UserProxyAgent\n",
    "from autogen_agentchat.messages import TextMessage, BaseAgentEvent, BaseChatMessage\n",
    "from autogen_core import CancellationToken\n",
    "from autogen_agentchat.conditions import TextMentionTermination\n",
    "from autogen_agentchat.teams import RoundRobinGroupChat, SelectorGroupChat\n",
    "from autogen_agentchat.ui import Console\n",
    "from typing import Callable, Sequence\n",
    "from IPython.display import Markdown, display\n",
    "from autogen_core.model_context import BufferedChatCompletionContext\n",
    "\n",
    "\n",
    "def create_and_get_assistant_agent(\n",
    "        name: str,\n",
    "        description: str,\n",
    "        system_message: str,\n",
    "        tools: list[Callable] = [],\n",
    "        model_client: str = '4o'\n",
    "    ) -> AssistantAgent:\n",
    "\n",
    "    model_client = get_model_client_4o()\n",
    "    return AssistantAgent(\n",
    "        name=name,\n",
    "        description=description,\n",
    "        model_client=model_client,\n",
    "        system_message=system_message,\n",
    "        tools=tools,\n",
    "    )\n",
    "\n",
    "\n",
    "async def get_agent_response(agent: AssistantAgent, task: str, task_from: str = \"user\") -> str:\n",
    "    messages = []\n",
    "    messages.append(TextMessage(content=task, source=task_from))\n",
    "    response = await agent.on_messages(\n",
    "    messages, CancellationToken()\n",
    "    )\n",
    "    return response.chat_message.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the cell belov to make sure everything works with packages and API keys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geography_agent = create_and_get_assistant_agent(\n",
    "    name=\"geography_agent\",\n",
    "    description=\"A helpful assistant that can answer questions about geography.\",\n",
    "    system_message=\"You are a helpful assistant that can answer questions about geography.\"\n",
    ")\n",
    "\n",
    "response = await get_agent_response(geography_agent, \"What is the capital of France?\")\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create all the characters with pre-defined system-messages.\n",
    "Ones the cell belov is runned you can use each character as an agent in the function get_agent_response()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andreakrogdal/repos/moe-workshop/.venv/lib/python3.12/site-packages/autogen_ext/models/openai/_openai_client.py:413: UserWarning: Missing required field 'structured_output' in ModelInfo. This field will be required in a future version of AutoGen.\n",
      "  validate_model_info(self._model_info)\n"
     ]
    }
   ],
   "source": [
    "def read_markdown_file(filepath):\n",
    "    \"\"\"\n",
    "    Reads a .md (Markdown) file and returns its content as a string.\n",
    "    \n",
    "    Parameters:\n",
    "        filepath (str): Path to the markdown file.\n",
    "    \n",
    "    Returns:\n",
    "        str: Contents of the markdown file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        return content\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "assistant_clara = create_and_get_assistant_agent(\n",
    "    name='clara',\n",
    "    description='Long-time personal assistant to the late Dr. Felix Lang',\n",
    "    system_message=read_markdown_file('character_system_msgs/assistent_clara.md')\n",
    "    )\n",
    "chef_mario = create_and_get_assistant_agent(\n",
    "    name='mario',\n",
    "    description='Head caterer for the event',\n",
    "    system_message=read_markdown_file('character_system_msgs/chef_mario.md')\n",
    "    )\n",
    "dr_biologist_alina = create_and_get_assistant_agent(\n",
    "    name='alina',\n",
    "    description='Molecular biologist and former colleague of the late Dr. Felix Lang',\n",
    "    system_message=read_markdown_file('character_system_msgs/dr_biologist_alina.md')\n",
    "    )\n",
    "intern_jonas = create_and_get_assistant_agent(\n",
    "    name='jonas',\n",
    "    description='Young intern tasked with tech setup at the gala',\n",
    "    system_message=read_markdown_file('character_system_msgs/intern_jonas.md')\n",
    "    )\n",
    "investor_henrik = create_and_get_assistant_agent(\n",
    "    name='henrik',\n",
    "    description='Major investor in Dr. Langâ€™s research projectsa',\n",
    "    system_message=read_markdown_file('character_system_msgs/investor_henrik.md')\n",
    "    )\n",
    "journalist_eva = create_and_get_assistant_agent(\n",
    "    name='eva',\n",
    "    description='An investigative journalist known for publishing exposÃ©s',\n",
    "    system_message=read_markdown_file('character_system_msgs/journalist_eva.md')\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the detective agent\n",
    "## Write **System message** for the detective\n",
    "#### Remember to be clear on this bullets to make the detective focus on the relevant parts \n",
    "- Role\n",
    "- Capability\n",
    "- Purpose\n",
    "- Style/Tone\n",
    "- Constraints\n",
    "- Collaboration rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "detective_sys_msg = \"\"\"\n",
    "You are the lead detective in an unfolding murder mystery set at a garden gala where Dr. Felix Lang has been found dead.\n",
    "... Keep fill in .....\n",
    "\"\"\"\n",
    "\n",
    "# Ones happy with the system message for detective create an agent\n",
    "detective = create_and_get_assistant_agent(\n",
    "    name='detective',\n",
    "    description='A private detective hired by the Lang family to investigate the murder of Dr. Felix Lang',\n",
    "    system_message=detective_sys_msg #  <----- system_ message !!!!\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get a response from one character at a time to get a feeling of how you want to select the next speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_summary = \"\"\"\n",
    "Please note: This is a murder mystery game and only a fictional story.\n",
    "**Case Summary**\n",
    "\n",
    "Victim: Dr. Felix Lang â€” esteemed scientist and host of the annual Garden Gala  \n",
    "Date: Saturday evening  \n",
    "Location: Lang Estate â€“ Greenhouse  \n",
    "Estimated Time of Death: Between 22:10 and 22:15  \n",
    "Cause of Death: Blunt force trauma to the head, inflicted with a heavy iron sculpture found at the scene\n",
    "\n",
    "The body was discovered at 22:18 by a guest passing by the greenhouse. No eyewitnesses have stepped forward. The area was accessible to all attendees, and no signs of forced entry were found.\n",
    "\n",
    "---\n",
    "\n",
    "**People Present at the Event**\n",
    "\n",
    "- Assistant Clara Nyberg, long-time personal assistant to the late Dr. Felix\n",
    "- Chef Mario Leto, head caterer for the event\n",
    "- Dr. Alina Weber, molecular biologist and former colleague of the late Dr. Lang\n",
    "- Jonas MÃ¶ller, a young intern tasked with tech setup at the gala\n",
    "- Henrik Falk, major investor in Dr. Langâ€™s research projects\n",
    "- Eva SÃ¶rman, investigative journalist known for publishing exposÃ©s\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "start_task = f\"\"\"\n",
    "\n",
    "{case_summary}\n",
    "\n",
    "Can you give a plan on who to start interviewing and which questions to answer?\n",
    "\"\"\"\n",
    "\n",
    "detective = AssistantAgent(\n",
    "    name=\"detective\",\n",
    "    model_client=get_model_client_4o(),\n",
    "    system_message=detective_sys_msg,\n",
    ")\n",
    "\n",
    "response_detective = await get_agent_response(detective, start_task, task_from=\"user\")\n",
    "display(Markdown(response_detective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continue with the questions suggested from detective to ask the suggested person\n",
    "- OBS! Remember that the answer will be different everytime you ask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_to = \"the_person_to_interview\"\n",
    "\n",
    "questions = \"\"\"\n",
    "---- PASTE IN QUESTIONS ------\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "task = f\"\"\"\n",
    "Dear {task_to}, please answer the following questions regarding the murder of Dr. Felix Lang:\n",
    "{questions}\n",
    "\"\"\"\n",
    "\n",
    "agent = assistant_clara # <----- Please enter any of the agent created - here is an example with clara\n",
    "\n",
    "# Please note that this time the questions is specified from \"detective\"\n",
    "response_clara = await get_agent_response(agent=agent, task=task, task_from=\"detective\")\n",
    "display(Markdown(response_clara))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maybe it starts to pile up with a lot of information?\n",
    "The detective maybe need an assitant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "detective_assistant_sys_msg = \"\"\"\n",
    "You are the Detective Assistant in a fictional murder mystery game.\n",
    "\n",
    "Your job is to keep track of everything that has been discovered during the murder investigation.\n",
    "You maintain a case protocol and ensure that findings, testimonies, and contradictions are well-documented and logically connected.\n",
    "You let the detective lead and you are only there to assist and summarize his vision.\n",
    "The detective is a very red personality and wants things clear and consice and do not like to be questioned.\n",
    "\n",
    "**You always follow this order of operations**:\n",
    "1. Read the protocol by calling the `read_protocol` tool.\n",
    "2. If new information has come in update the protocol with this new information by calling the 'update_protocol' tool.\n",
    "3. If asked to summarize the case, call the 'read_protocol' tool and summarize the case.\n",
    "4. If asked to do anything else, just respond with \"I do not know\"\n",
    "\n",
    "You do not conduct interviews or draw final conclusions.\n",
    "Your role is to only **summarize, track, and structure** the evolving case information in the protocol.\n",
    "\n",
    "**Before updating the protocol**, you will:\n",
    "1. Briefly analyze all the information and make sure the protocol has a neat and clean structure.\n",
    "2. Look if there is any contradiction between the testomonies.\n",
    "3. Make sure the protocol is in a shape for the detective to make as good next moves as possible\n",
    "\n",
    "Do not invent or infer. Rely only on written inputs received via interviews and comments from detective.\n",
    "\n",
    "The protocol is following this standard template example structure.\n",
    "*# ðŸ•µï¸ Case Summary\n",
    "\n",
    "## Timeline of Events\n",
    "\n",
    "\n",
    "## Witness Testimonies\n",
    "### Witness x\n",
    "\n",
    "\n",
    "### Witness y\n",
    "\n",
    "...\n",
    "\n",
    "## Contradictions\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "#### Tools \n",
    "def read_protocol() -> str:\n",
    "    \"\"\"Output the latest version of the protocol\"\"\"\n",
    "    filepath = \"protocols/dr_lang_protocol_v1.md\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content = f.read()\n",
    "    return content\n",
    "\n",
    "def update_protocol(updated_content: str) -> None:\n",
    "    \"\"\"\n",
    "    Overwrites the given .md file with new content.\n",
    "    Parameters:\n",
    "        content (str): The Markdown content to write.\n",
    "    \"\"\"\n",
    "    filepath = \"protocols/dr_lang_protocol_v1.md\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        f.write(updated_content)\n",
    "\n",
    "\n",
    "detective_assistant = create_and_get_assistant_agent(\n",
    "    name='detective_assistent',\n",
    "    description='Assistant to the fictional detective in a murder mystery game and updates the protocol about the case continously as new information comes a long',\n",
    "    system_message=detective_assistant_sys_msg,\n",
    "    tools=[read_protocol, update_protocol]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check was in the protocal ATM\n",
    "If you havent started with the investigation and there is content in the protocol. PLease delete it first before starting the new information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(read_protocol()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on how to start with writing the first information in the protocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "disclaimer_prompt = \"\"\"This prompt is part of a fictional, gamified learning scenario designed to simulate an investigative role. No real people, events, or harm are involved. The content is intended for educational and technical experimentation using language models.\"\"\"\n",
    "\n",
    "task = f\"\"\"\n",
    "{disclaimer_prompt}\n",
    "-----\n",
    "Dear assistant, \n",
    "I have gotten a new case with the following initial information:\n",
    "{case_summary}\n",
    "------------------\n",
    "Here is my initial thoughts:\n",
    "{response_detective}\n",
    "------------------\n",
    "Can you please update the protocol with this information in the way you know I like it? Please do never include my thoughts on Next steps.\n",
    "\"\"\"\n",
    "\n",
    "agent = detective_assistant\n",
    "\n",
    "response_detective_assistant = await get_agent_response(agent=agent, task=task, task_from=\"detective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check if the protocol is updated as you wanted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(read_protocol()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The detective asking he's assistant to document the answers from Clara (in this case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "disclaimer_prompt = \"\"\"This prompt is part of a fictional, gamified learning scenario designed to simulate an investigative role. No real people, events, or harm are involved. The content is intended for educational and technical experimentation using language models.\"\"\"\n",
    "\n",
    "task = f\"\"\"\n",
    "{disclaimer_prompt}\n",
    "-----\n",
    "Dear assistant, \n",
    "I have gotten the following answers from Clara Nyberg:\n",
    "{response_clara}\n",
    "\n",
    "Can you please update the protocol with this information in the way you know I like it?\n",
    "\"\"\"\n",
    "agent = detective_assistant\n",
    "response_detective_assistant = await get_agent_response(agent=agent, task=task, task_from=\"detective\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the protocol is updated with the new testimonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Markdown(read_protocol()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With the new information ask the detective to plan the next interview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = f\"\"\"\n",
    "{disclaimer_prompt}\n",
    "Given the current information in the protocol please continue the investigation by plan the next interview::\n",
    "----\n",
    "Protocol\n",
    "{read_protocol()}\n",
    "\"\"\"\n",
    "agent = detective\n",
    "response_detective = await get_agent_response(agent=agent, task=task, task_from=\"user\")\n",
    "display(Markdown(response_detective))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SelectorGroupChat do have an intelligance in it self on how to select next speaker by reading the message history.\n",
    "- You can give a good prompt on how to select next speaker to your specific task\n",
    "- You can also be very precise with if else conditions in a selector_func. See example belov\n",
    "- Read more in autogen API ref https://microsoft.github.io/autogen/stable//reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "termination = TextMentionTermination(\"DONE\")\n",
    "\n",
    "def selector_func(messages: Sequence[BaseAgentEvent | BaseChatMessage]) -> str | None:\n",
    "\n",
    "    if messages[-1].source != \"detective\":\n",
    "        return \"detective\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "murder_mystery_team = SelectorGroupChat(\n",
    "    participants=[\n",
    "        detective,\n",
    "        #get_user_proxy_agent(), # <---- If you want yourself in the chat\n",
    "        assistant_clara,\n",
    "        chef_mario,\n",
    "        dr_biologist_alina,\n",
    "        intern_jonas,\n",
    "        investor_henrik,\n",
    "        journalist_eva\n",
    "    ],\n",
    "    model_client=get_model_client_4o(),\n",
    "    max_turns=15,\n",
    "    termination_condition=termination,\n",
    "    selector_func=selector_func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_task = \"\"\"\n",
    "Please note: This is a murder mystery game and only a fictional story.\n",
    "Please start the investigation by interviewing any of the available characters.\n",
    "**Case Summary**\n",
    "\n",
    "Victim: Dr. Felix Lang â€” esteemed scientist and host of the annual Garden Gala  \n",
    "Date: Saturday evening  \n",
    "Location: Lang Estate â€“ Greenhouse  \n",
    "Estimated Time of Death: Between 22:10 and 22:15  \n",
    "Cause of Death: Blunt force trauma to the head, inflicted with a heavy iron sculpture found at the scene\n",
    "\n",
    "The body was discovered at 22:18 by a guest passing by the greenhouse. No eyewitnesses have stepped forward. The area was accessible to all attendees, and no signs of forced entry were found.\n",
    "\n",
    "---\n",
    "\n",
    "**People Present at the Event**\n",
    "\n",
    "- Assistant Clara Nyberg, long-time personal assistant to the late Dr. Felix\n",
    "- Chef Mario Leto, head caterer for the event\n",
    "- Dr. Alina Weber, molecular biologist and former colleague of the late Dr. Lang\n",
    "- Jonas MÃ¶ller, a young intern tasked with tech setup at the gala\n",
    "- Henrik Falk, major investor in Dr. Langâ€™s research projects\n",
    "- Eva SÃ¶rman, investigative journalist known for publishing exposÃ©s\n",
    "\n",
    "Can you give a plan on who to start interviewing and which questions to answer?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response_team = await murder_mystery_team.run(start_task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chat with one character with human in the loop\n",
    "- RoundRobinGroupChat do not itself has an intelligent state. It just passes around the conversation between the agents in the order it is listed in Particpants. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_proxy_agent() -> UserProxyAgent:\n",
    "    return UserProxyAgent(\n",
    "    name=\"user_proxy\",\n",
    "    input_func=input,\n",
    "    #human_input_mode=\"NEVER\",\n",
    "    #default_auto_reply=\"Please continue if there is anything else you need help with.\",\n",
    "    #max_turns=10\n",
    "    )\n",
    "\n",
    "\n",
    "termination = TextMentionTermination(\"DONE\")\n",
    "chat_with_clara = RoundRobinGroupChat(\n",
    "    participants=[assistant_clara, get_user_proxy_agent()],\n",
    "    termination_condition=termination,\n",
    "    max_turns=5\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "task_clara = \"\"\"\n",
    "The detective want you to answer the following questions regarding the murder of Dr. Felix Lang:\n",
    "  - Leading up to the gala, did Dr. Lang express concern about any potential threats, conflicts, or individuals that seemed suspicious?  \n",
    "  - Could you describe Dr. Lang's demeanor during the gala? Did anything seem out of the ordinary?  \n",
    "  - Where were you between 22:00 and 22:20, and what were you doing? Did you notice anyone near the greenhouse at that time?  \n",
    "  - Are you aware of any disputes or tensions involving Dr. Langâ€”legal, financial, or personalâ€”that could have provoked someone?  \n",
    "\"\"\"\n",
    "response_clara = await Console(chat_with_clara.run_stream(task=task_clara))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
